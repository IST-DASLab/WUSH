{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import random \n",
    "import numpy as np\n",
    "def fix_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "fix_seed(0)\n",
    "MODEL_DIR = \"quantized_models/Llama-3.2-3B-mxfp-w4-a4-RTN-wush\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # change to your full path if needed\n",
    "def pick_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "device = pick_device()\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "\n",
    "# Model\n",
    "# If this folder is quantized with custom kernels and fails to load, see notes below.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    device_map=\"cuda\",\n",
    "    trust_remote_code=True,  # keep True if the repo/folder has custom modeling code\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e72f71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548a3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model.model.layers[2].mlp.gate_proj\n",
    "\n",
    "print(\"Parameters:\")\n",
    "for name, p in mod.named_parameters(recurse=False):\n",
    "    print(\" \", name, p.shape, p.dtype, p.device)\n",
    "    # print(\"    \", p)\n",
    "\n",
    "print(\"\\nBuffers:\")\n",
    "for name, b in mod.named_buffers(recurse=False):\n",
    "    print(\" \", name, b.shape if hasattr(b, \"shape\") else type(b), b.dtype if torch.is_tensor(b) else \"\")\n",
    "    print(\"    \", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8590c0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9107d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is quantization-aware training?Assistant:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "if device == \"cpu\":\n",
    "    pass\n",
    "elif device == \"cuda\":\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "else:\n",
    "    # CUDA: if device_map=\"auto\" was used, inputs can stay on CPU; HF will dispatch.\n",
    "    # But moving to CUDA explicitly is also fine if the whole model is on one GPU.\n",
    "    # We'll keep it simple and leave as-is.\n",
    "    pass\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=12,\n",
    "        do_sample=False,\n",
    "        temperature=0.8,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f46c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def get_wikitext2(tokenizer: AutoTokenizer,  sequence_length: int):\n",
    "    test_dataset_raw = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    test_dataset_tok = tokenizer(\"\\n\\n\".join(test_dataset_raw[\"text\"]), return_tensors=\"pt\").input_ids\n",
    "    num_test_sequences = test_dataset_tok.numel() // sequence_length\n",
    "    test_loader = []\n",
    "    for i in range(num_test_sequences):\n",
    "        test_loader.append(test_dataset_tok[:, i * sequence_length : (i + 1) * sequence_length])\n",
    "    return test_loader\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_perplexity(model, data, batch_size: int = 1):\n",
    "    num_samples = len(data)\n",
    "    device = next(model.parameters()).device\n",
    "    # Running estimate of negative log-likelihood\n",
    "    nll_running = 0\n",
    "    # Number of tokens processed to far\n",
    "    tokens_processed = 0\n",
    "    # Loop through each batch\n",
    "    for i in trange(0, num_samples, batch_size, desc=\"Computing perplexity\", leave=False):\n",
    "        j = min(i + batch_size, num_samples)\n",
    "        inputs = torch.cat(data[i:j]).to(device)\n",
    "        # Forward pass through the model\n",
    "        lm_logits = model(inputs).logits\n",
    "        # Shift logits and labels for next token prediction\n",
    "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
    "        shift_labels = inputs[:, 1:]\n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(shift_logits.reshape(-1, shift_logits.size(-1)), shift_labels.reshape(-1))\n",
    "        # Calculate negative log likelihood\n",
    "        a = shift_labels.numel() / (tokens_processed + shift_labels.numel())\n",
    "        b = tokens_processed / (tokens_processed + shift_labels.numel())\n",
    "        nll_running = a * loss + b * nll_running\n",
    "        # Update number of processed tokens\n",
    "        tokens_processed += shift_labels.numel()\n",
    "    # Compute perplexity\n",
    "    ppl = nll_running.exp().item()\n",
    "    return ppl\n",
    "\n",
    "\n",
    "eval_data = get_wikitext2(tokenizer, 2048)\n",
    "ppl = compute_perplexity(model, eval_data)\n",
    "print(f\"Perplexity: {ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8d2fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm_eval.models.huggingface import HFLM\n",
    "import lm_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = HFLM(\n",
    "            pretrained=model, \n",
    "            tokenizer=tokenizer, \n",
    "            batch_size=64,\n",
    "            max_length=4096, # from open LLM openllm\n",
    "        )\n",
    "task_manager = lm_eval.tasks.TaskManager()\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cebb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_results = lm_eval.simple_evaluate(\n",
    "                    model=lm,\n",
    "                    tasks=\"winogrande\",\n",
    "                    batch_size=64,\n",
    "                    task_manager=task_manager,\n",
    "                    # confirm_run_unsafe_code=True,\n",
    "                )[\"results\"]\n",
    "results.append(task_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43daa8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_results = lm_eval.simple_evaluate(\n",
    "                model=lm,\n",
    "                tasks=\"hellaswag\",\n",
    "                num_fewshot=10,\n",
    "                batch_size= 64,\n",
    "                task_manager=task_manager,\n",
    "            )[\"results\"]\n",
    "results.append(task_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_results = lm_eval.simple_evaluate(\n",
    "                    model=lm,\n",
    "                    tasks=\"mmlu_cot_llama\",\n",
    "                    batch_size=32,\n",
    "                    apply_chat_template=True,\n",
    "                    fewshot_as_multiturn=True,\n",
    "                    task_manager=task_manager,\n",
    "                )[\"results\"]\n",
    "results.append(task_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_results = lm_eval.simple_evaluate(\n",
    "                model=lm,\n",
    "                tasks=\"gsm8k_llama\",\n",
    "                batch_size=32,\n",
    "                apply_chat_template=True,\n",
    "                fewshot_as_multiturn=True,\n",
    "                task_manager=task_manager,\n",
    "            )[\"results\"]\n",
    "results.update(task_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-wush",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
